{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left\">\n",
    "            <h1 style=\"width:450px\">CASA0006 Practical 3: Tree-based methods</h1>\n",
    "</div>\n",
    "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "\n",
    "In this workshop, we will achieve the following objectives:\n",
    "1. Review weekly quiz;\n",
    "1. Build three models to predict the daily bicycle rental based on metrics of R2, including CART, random forest, and XGBoost;\n",
    "1. Compare the predictive performance and permutation feature importance of these three models.\n",
    "\n",
    "Remember that *Practice makes perfect.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, some codes are missing and marked with ```??```.\n",
    "\n",
    "Please replace ```??``` with the correct codes, using the hints and context. The solution will be given in the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*\\* Review the weekly quiz \\*\\*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the relevant libraries. \n",
    "\n",
    "* `pandas` for data import and handling;\n",
    "* `matplotlib`;\n",
    "* `numpy`;\n",
    "* `sklearn`;\n",
    "* `xgboost` for XGBoost models.\n",
    "* `rfpimp` for permutation feature importance.\n",
    "\n",
    "**Run the script below to get started.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# CART\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# feature importance\n",
    "import rfpimp\n",
    "\n",
    "# xgboost\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option('display.max_rows', 300) # specifies number of rows to show\n",
    "pd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\n",
    "plt.style.use('ggplot') # specifies that graphs should use ggplot styling\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the library version before we start\n",
    "print(\"xgboost version:{}\".format(xgboost.__version__))\n",
    "print(\"sklearn version:{}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you will use relates to daily counts of rented bicycles from the bicycle rental company Capital-Bikeshare in Washington D.C., along with weather and seasonal information. The goal here is to predict how many bikes will be rented depending on the weather and the day. The original data can be downloaded from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).\n",
    "\n",
    "The dataset used in this workshop has been slightly processed by Christoph Molnar using the processing R-script from this [Github repository](https://github.com/christophM/interpretable-ml-book/blob/master/R/get-bike-sharing-dataset.R). Here, the dataset is provided as a csv file on Moodle.\n",
    "\n",
    "Here is a list of the variables in the dataset:\n",
    "\n",
    "- Count of bicycles including both casual and registered users. The count is used as the response in the regression task.\n",
    "- Indicator of the season, either spring, summer, fall or winter.\n",
    "- Indicator whether the day was a holiday or not.\n",
    "- The year: either 2011 or 2012.\n",
    "- Number of days since the 01.01.2011 (the first day in the dataset). This predictor was introduced to take account of the trend over time.\n",
    "- Indicator whether the day was a working day or weekend.\n",
    "- The weather situation on that day. One of:\n",
    "  - **'GOOD'**: including clear, few clouds, partly cloudy, cloudy\n",
    "  - **'MISTY'**: including mist + clouds, mist + broken clouds, mist + few clouds, mist\n",
    "  - **'RAIN/SNOW/STORM'**: including light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds, heavy rain + ice pallets + thunderstorm + mist, snow + mist\n",
    "- Temperature in degrees Celsius.\n",
    "- Relative humidity in percent (0 to 100).\n",
    "- Wind speed in km/h.\n",
    "\n",
    "We will use Pandas package to load and explore this dataset:\n",
    "\n",
    "- Import the Boston housing dataset as a Pandas dataframe (call it `bike_rental`)\n",
    "- Inspect the data\n",
    "- Calculate summary statistics on all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rental = pd.read_csv('https://raw.githubusercontent.com/huanfachen/Spatial_Data_Science/main/Dataset/daily_count_bike_rental.csv')\n",
    "# drop the year variable as it is not useful\n",
    "bike_rental = bike_rental.drop(['yr'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rental.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `bike_rental`, there are two data types: categorical (aka `object`), and numerical (including `int64` and `float64`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a few rows of this dataset\n",
    "bike_rental.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting categorical variables\n",
    "\n",
    "Before undertaking regression, we need to handle categorical variables, as ML models usually can't deal with *strings* or *categories*.\n",
    "\n",
    "While there are other ways of handling categorical variables in machine learning, we will stick to one-hot encoding, as it is easy to understand and appropriate for medium-size datasets (sample size <10000).\n",
    "\n",
    "First, we need to convert categorical variables into dummy/indicator variables, using `One-Hot Encoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rentail_numeric = pd.get_dummies(bike_rental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the new dataFrame\n",
    "bike_rentail_numeric.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that, a cateogircal variable of K categories or levels, usually enters a regression as a sequence of K-1 dummy variables. The level that is left out becomes the reference level, and this is important for interpreting the regression model.\n",
    "\n",
    "Here we manually choose the reference level for each categorical variable and exclude them from the DataFrame. You can change the reference levels if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rental_final = bike_rentail_numeric.drop(['season_SPRING', 'mnth_JAN', 'holiday_NO HOLIDAY', 'weekday_MON', 'workingday_WORKING DAY', 'weathersit_GOOD'], axis=1)\n",
    "\n",
    "# double check the result\n",
    "bike_rental_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into random train and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `train_test_split` will split the data according to a 75:25 split. Other proportions can be specified, check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details.\n",
    "\n",
    "Remember that the split should be random in order to avoid selection bias. Here, we set random_state=100 to guarantee reproducibility.\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "The first argument of this function:\n",
    "\n",
    "```\n",
    "*arrays: sequence of indexables with same length / shape[0]\n",
    "Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "```\n",
    "\n",
    "The output of this function:\n",
    "```\n",
    "splitting: list, length=2 * len(arrays)\n",
    "List containing train-test split of inputs.\n",
    "```\n",
    "\n",
    "Here we input two dataframes (X and Y) and will get four outputs (train_x, test_x, train_y, test_y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_split = 100\n",
    "train_x, test_x, train_y, test_y = train_test_split(bike_rental_final.drop(['cnt'], axis = 1), bike_rental_final.cnt, random_state=random_state_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the rows and columns of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "# check the index of train_x and train_y - they should be identical. The index indicates which rows from the original data.\n",
    "\n",
    "print(train_x.index.identical(train_y.index))\n",
    "print(test_x.index.identical(test_y.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression models, we will use the training set to train the model and select hyperparameters. The testing set is only used to report the performance of the finalised model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a CART to predict the bicycle rental, following two steps:\n",
    "\n",
    "1. Tune the hyperparameters of max_depth and min_samples_split using cross validation;\n",
    "2. Train a final model based on the tuned hyperparameters and evaluate the model performance.\n",
    "\n",
    "Here, we focus on two hyperparameters that control the stopping criteria of the model training: max_depth and min_samples_split.\n",
    "\n",
    "*max_depth* refers to the maximal depth (or height) of a tree. In sklearn, if this value is not specified, the nodes would be expanded until all leaves contain samples of the same y value or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "Intuition about max_depth: the larger the maximum tree depth, the better fitness of the model on the training data and thus the higher possibility of overfitting.\n",
    "\n",
    "Ideally, we want the maximum tree height to be neither too small or too large.\n",
    "\n",
    "The documentation is as follows:\n",
    "\n",
    "```\n",
    "max_depth: int, default=None\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "```\n",
    "\n",
    "*min_samples_split* refers to the minimum number of samples required to split a node. If a node contains less samples than min_samples_split, this node becomes a leaf node and would not split.\n",
    "\n",
    "The smaller value of min_samples_split, the better fitness of the model and thus the higher possibility of overfitting.\n",
    "\n",
    "The documentation is as follows (note that the value can be either integer or float, but the meaning is different)\n",
    "\n",
    "```\n",
    "min_samples_split: int or float, default=2\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "If int, then consider min_samples_split as the minimum number.\n",
    "\n",
    "If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the hyperparameters using cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ```GridSearchCV``` to find the optimal combination of max_depth and min_samples_split. \n",
    "\n",
    "Note that in grid search, all combinations of hyperparameters will be tested and compared. In this case, 25 (5*5) different combinations of hyperparameters will be tested.\n",
    "\n",
    "**One question - how to determine the potential values of max_depth and min_samples_split?**\n",
    "\n",
    "For min_samples_split, as it is a positive integer value and the default value is 2, we can set the range as [2,4,6,8,10].\n",
    "\n",
    "For max_depth, we can build a decision tree using default settings and check the tree depth, then choose the range around this tree depth.\n",
    "\n",
    "The code below shows that the tree depth of a CART with default settings is 25, so we can set the range as [10,20,30,40,50]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a CART using default settings\n",
    "cart_default = DecisionTreeRegressor(random_state=0)\n",
    "cart_default.fit(train_x, train_y)\n",
    "# print the tree depth\n",
    "print(\"Tree depth: {}\".format(cart_default.get_depth()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values of max_depth and min_samples_split\n",
    "hyperparameters = {'max_depth':[10,20,30,40,50], 'min_samples_split':[2,4,6,8,10]}\n",
    "\n",
    "randomState_dt = 10000\n",
    "dt = DecisionTreeRegressor(random_state=randomState_dt)\n",
    "\n",
    "# cv=5 by default, which means 5-fold cross-validation\n",
    "clf = GridSearchCV(dt, hyperparameters)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "# we can query the best parameter value and its accuracy score\n",
    "print (\"The best parameter value is: \")\n",
    "print (clf.best_params_)\n",
    "print (\"The best score is: \")\n",
    "print (clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the final CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_final = DecisionTreeRegressor(max_depth=clf.best_params_['max_depth'], min_samples_split=clf.best_params_['min_samples_split'], random_state=randomState_dt)\n",
    "dt_final.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 on the training data:\")\n",
    "print(dt_final.score(X=train_x, y=train_y))\n",
    "print(\"R2 on the testing data:\")\n",
    "print(dt_final.score(X=test_x, y=test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the R2 on the testing data is much lower than that on the training data. This indicates the overfitting problem, meaning that the model fits very well to the training data but doesn't generalise well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE on the training data:\")\n",
    "print(mean_squared_error(train_y, dt_final.predict(train_x), squared=False))\n",
    "print(\"RMSE on the testing data:\")\n",
    "print(mean_squared_error(test_y, dt_final.predict(test_x), squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is in the same unit of the y variable (bicycle rental). On the testing data, on average, the model's prediction deviates from the true bicycle rental by 922.\n",
    "\n",
    "If you want to know the relative error, you can compute the normalised RMSE by dividing the mean true value, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normalised RMSE on the training data:\")\n",
    "print(mean_squared_error(train_y, dt_final.predict(train_x), squared=False)/np.mean(train_y))\n",
    "print(\"Normalised RMSE on the testing data:\")\n",
    "print(mean_squared_error(test_y, dt_final.predict(test_x), squared=False)/np.mean(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some attributes of the tree\n",
    "print(\"Tree depth:{}\".format(dt_final.??()))\n",
    "print(\"Number of leaves:{}\".format(dt_final.get_n_leaves()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to inspect and interpret the CART. \n",
    "\n",
    "One way is to visualise the whole tree and check each node and split. However, this is not easy for a tree with 10 layers and over 100 leaves.\n",
    "\n",
    "Another way is to check the relative importance of each feature, for example, using the permutation feature importance.\n",
    "\n",
    "To do this, we will use the **rfpimp** package. \n",
    "\n",
    "**rfpimp** is short for random forest permutation importance. It is a library that provides feature importances, based upon the permutation importance strategy, for general scikit-learn models and implementations specifically for random forest out-of-bag scores. Built by Terence Parr and Kerem Turgutlu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **rfpimp.importances** function to compute the feature importance. According to [its documentation](https://github.com/parrt/random-forest-importances/blob/master/src/rfpimp.py):\n",
    "\n",
    "```\n",
    "Compute permutation feature importances for scikit-learn models using a validation set.\n",
    "Given a Classifier or Regressor in model and validation X and y data, return a data frame with columns.\n",
    "Feature and Importance sorted in reverse order by importance.\n",
    "The validation data is needed to compute model performance measures (accuracy or R^2). The model is not retrained.\n",
    "```\n",
    "\n",
    "So the importance of a feature x is defined as the difference of the R^2 model score (computed by **dt_final.score()**) after shuffling x and without shuffling x.\n",
    "\n",
    "The feature importance is mostly positive or a negative value very close to zero, as shuffling a feature is not expected to greatly improve the model performance.\n",
    "\n",
    "The feature importance is usually used to compare the relative importance of features. It is not recommended to interpret the absolute value of this importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the importances function returns a dataframe of two columns, Feature and Importance, and the features are ranked from the most important to the least.\n",
    "imp = rfpimp.importances(dt_final, test_x, test_y)\n",
    "print(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is a barplot of the feature importance\n",
    "viz = rfpimp.plot_importances(imp)\n",
    "viz.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance above shows that the most important features are **days_since_2011**, **temperature**, and **humidity**.\n",
    "\n",
    "Later on, we will compare the feature importance of diffferent models - CART, RF, and GBDT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and interpreting a random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will use the random forest method to predict the daily bicycle rental. We will follow a simple workflow as CART: tune the hyperparameters and then train the final model.\n",
    "\n",
    "RF has two groups of hyperparameters:\n",
    "\n",
    "1. The hyperparameters of decision trees, such as max_depth, min_samples_split;\n",
    "2. The configurations of RF, such as the number of trees (called **n_estimators**). By default, **n_estimators** is set as 100. Some intuition is that the larger n_estimators, the more stable the model performance and the longer computing time. This is because the computing time is a linear function of the number of trees.\n",
    "\n",
    "The default value of 100 trees is good for most applications, so we don't tune this hyperparameter here.\n",
    "\n",
    "As with CART, we will focus on the tuning of max_depth and min_samples_split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values of max_depth and min_samples_split\n",
    "hyperparameters = {'max_depth':[10,20,30,40,50], '??':[2,4,6,8,10]}\n",
    "\n",
    "randomState_dt = 10000\n",
    "rf = RandomForestRegressor(random_state=randomState_dt)\n",
    "\n",
    "# cv=5 by default, which means 5-fold cross-validation\n",
    "clf = GridSearchCV(rf, ??)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "# we can query the best parameter value and its accuracy score\n",
    "print (\"The best parameter value is: \")\n",
    "print (clf.best_params_)\n",
    "print (\"The best score is: \")\n",
    "print (clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the final RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_final = RandomForestRegressor(max_depth=clf.best_params_['max_depth'], min_samples_split=clf.best_params_['min_samples_split'], random_state=randomState_dt)\n",
    "rf_final.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 on the training data:\")\n",
    "print(rf_final.score(X=train_x, y=train_y))\n",
    "print(\"R2 on the testing data:\")\n",
    "print(rf_final.score(X=test_x, y=test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So - what do you think about the comparison of R2 on the training and testing data?\n",
    "\n",
    "The R2 on the testing data of RF is much higher than CART, meaning that RF is much better at making prediction for the unseen data and has a better generality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE on the training data:\")\n",
    "print(mean_squared_error(train_y, rf_final.??(??), squared=False))\n",
    "print(\"RMSE on the testing data:\")\n",
    "print(mean_squared_error(test_y, rf_final.??(??), squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with CART, it is not useful to visualise the trees in a random forest.\n",
    "\n",
    "Calculate and plot the feature importance of the RF model.\n",
    "\n",
    "Although the feature importance value differs from the CART, the order of the top-ranking variables is the same.\n",
    "\n",
    "This indicates that these variables are indeed very important for the prediction of bike rental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = rfpimp.importances(rf_final, test_x, test_y)\n",
    "print(imp)\n",
    "viz = rfpimp.plot_importances(imp)\n",
    "viz.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and interpreting a XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will use the XGBoost method to predict the daily bicycle rental. We will follow a simple workflow as CART and RF: tune the hyperparameters and then train the final model.\n",
    "\n",
    "XGBoost is not part of the sklearn library but a separate library. It provides the sklearn API so that we can use XGboost in the same way as RandomForestRegressor of sklearn.\n",
    "\n",
    "XGBoost has two groups of hyperparameters:\n",
    "\n",
    "1. The hyperparameters of decision trees, such as max_depth, min_samples_split;\n",
    "2. The configurations of XGBoost, such as the number of trees (called **n_estimators**) and regularisation hyperparameters.\n",
    "\n",
    "Here are a list of potentially important hyperparameters for XGBoost, recommended by [this post](https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663):\n",
    "\n",
    "1. `max_depth`: The maximum depth per tree. A deeper tree might increase the performance, but also the complexity and chances to overfit. The value must be an integer greater than 0. Default is 6.\n",
    "\n",
    "1. `learning_rate`: The learning rate determines the step size at each iteration while your model optimizes toward its objective. A low learning rate makes computation slower, and requires more rounds to achieve the same reduction in residual error as a model with a high learning rate. But it optimizes the chances to reach the best optimum. The value must be between 0 and 1. Default is 0.3.\n",
    "\n",
    "1. `n_estimators`: The number of trees in the ensemble. Equivalent to the number of boosting rounds. The value must be an integer greater than 0. Default is 100.\n",
    "\n",
    "1. `colsample_bytree`: Represents the fraction of columns to be randomly sampled for each tree. It might improve overfitting.\n",
    "The value must be between 0 and 1. Default is 1.\n",
    "\n",
    "1. `subsample`: Represents the fraction of observations to be sampled for each tree. A lower values prevent overfitting but might lead to under-fitting.\n",
    "The value must be between 0 and 1. Default is 1.\n",
    "\n",
    "1. `alpha (reg_alpha)`: L1 regularization on the weights (Lasso Regression). When working with a large number of features, it might improve speed performances. It can be any integer. Default is 0.\n",
    "\n",
    "1. `lambda (reg_lambda)`: L2 regularization on the weights (Ridge Regression). It might help to reduce overfitting. It can be any integer. Default is 1.\n",
    "\n",
    "1. `gamma`: Gamma is a pseudo-regularisation parameter (Lagrangian multiplier), and depends on the other parameters. The higher Gamma is, the higher the regularization. It can be any integer. Default is 0.\n",
    "\n",
    "The last three hyperparameters - alpha, lambda, and gamma - are related to the regularization of the model. In machine learning, regularization is a number of techniques to reduce the model complexity and the chances of overfitting.\n",
    "\n",
    "If you want to know more about these hyperparameters, you can read [the XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html).\n",
    "\n",
    "Here, we will focus on the tuning of max_depth and n_estimators.\n",
    "\n",
    "One note is that this grid search may take several minutes, which is longer than CART or random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values of max_depth and min_samples_split\n",
    "hyperparameters = {'??':[10,20,30,40,50], 'n_estimators':[50,100,150,200,250]}\n",
    "\n",
    "randomState_xgb = 125\n",
    "xgb = XGBRegressor(random_state=randomState_xgb)\n",
    "\n",
    "# cv=5 by default, which means 5-fold cross-validation\n",
    "gscv_xgb = GridSearchCV(xgb, hyperparameters)\n",
    "\n",
    "gscv_xgb.fit(train_x, train_y)\n",
    "\n",
    "# we can query the best parameter value and its accuracy score\n",
    "print (\"The best parameter value is: \")\n",
    "print (gscv_xgb.best_params_)\n",
    "print (\"The best score is: \")\n",
    "print (gscv_xgb.??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the final XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final = XGBRegressor(max_depth=gscv_xgb.best_params_['max_depth'], n_estimators=gscv_xgb.best_params_['n_estimators'], random_state=randomState_xgb)\n",
    "xgb_final.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 on the training data:\")\n",
    "print(??.score(X=train_x, y=train_y))\n",
    "print(\"R2 on the testing data:\")\n",
    "print(xgb_final.score(X=??, y=test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So - what do you think about the comparison of R2 on the training and testing data?\n",
    "\n",
    "The R2 on the testing data of XGBoost is much higher than CART and similar to RF, meaning that XGBoost is much better at making prediction for the unseen data and has a better generality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE on the training data:\")\n",
    "print(mean_squared_error(train_y, xgb_final.predict(train_x), squared=False))\n",
    "print(\"RMSE on the testing data:\")\n",
    "print(mean_squared_error(test_y, xgb_final.??(test_x), squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with CART, it is not useful to visualise the trees in a random forest.\n",
    "\n",
    "Calculate and plot the feature importance of the XGboost model.\n",
    "\n",
    "Although the feature importance value differs from the CART, the order of the top-ranking variables is the same.\n",
    "\n",
    "This indicates that these variables are indeed very important for the prediction of bike rental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = rfpimp.importances(xgb_final, ??, ??) # permutation\n",
    "print(imp)\n",
    "viz = rfpimp.plot_importances(imp)\n",
    "viz.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will collate and compare the model performance of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of models\n",
    "list_name_models = ['CART', 'RF', 'XGBoost']\n",
    "# use the models from above\n",
    "list_reg_models = [dt_final, rf_final, xgb_final]\n",
    "\n",
    "dict_models = dict()\n",
    "\n",
    "for name, model in zip(list_name_models, list_reg_models):\n",
    "    dict_models[name] = [model.score(train_x, train_y), model.score(test_x, test_y), model.score(train_x, train_y) - model.score(test_x, test_y)]\n",
    "\n",
    "# transform dict_models to dataframe\n",
    "df_models = pd.DataFrame.from_dict(dict_models, orient='index', columns=['R2_train_data', 'R2_test_data', 'R2_diff'])\n",
    "df_models\n",
    "# you can then export df_models as a csv file and use it in MS Excel or Word\n",
    "# df_models.to_csv(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result shows that the random forest model has a higher R2 on the testing data than CART and XGBoost, as well as a smaller R2 difference between training and testing data. \n",
    "\n",
    "This indicates that the random forest model is less subject to the overfitting issue and has a better generalisation.\n",
    "\n",
    "The XGBoost also has a less extent of overfitting than CART. \n",
    "\n",
    "For this reason, it is uncommon to use CART to directly make predictions. Rather, CART is used to construct RF or GBDT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this workshop, we have used decision trees, random forest, and gradient boosting decision trees to make predictions for the daily bicycle rental.\n",
    "\n",
    "We compare the model performance and the feature importance. Overall, the RF and GBDT models have better predictive performance and better generalisation than CART.\n",
    "\n",
    "The trend of relative feature importance is consistent across the three models. The most important features are days since 2011, temperature, and humidity.\n",
    "\n",
    "In the later weeks, we will demonstrate the use of these models for classification tasks and the scalability of these models for big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and recommendations:\n",
    "\n",
    "1. Permutation importance: [this chapter](https://christophm.github.io/interpretable-ml-book/feature-importance.html) is recommended, which is part of the E-book 'Interpretable machine learning'.\n",
    "1. [The documentation](https://xgboost.readthedocs.io/en/latest/parameter.html) of XGBoost hyperparameters (note that they use the term *parameters* in this webpage).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
